{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0010-SRT-pilot\n",
    "\n",
    "This experiment was conducted by Kathryn Schuler in collaboration with Elissa Newport (advisor), Darlene Howard, & Jim Howard.  The lab manager at the time of running was Katherine Olson and the research assistant who collected the data was Jason Sotomayor.  The data was collected at Georgetown University from October 23, 2012 to November 15, 2012.\n",
    "\n",
    "This is a simple serial reaction time tast (SRT) in which participants see a screen with 4 circles.  When a circle fills, they must touch the corresponding keyboard button (z, x, n, m) as quickly and accurately as possible.  We measure their accuracy and reaction time over 20 blocks of 80 trials.  Faster reaction times reveal learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [Materials and method](#Materials-and-method)\n",
    "    - [Subjects](#Subjects)\n",
    "    - [Materials](#Materials)\n",
    "    - [Procedure](#Procedure)\n",
    "- [Results and analysis](#Results-and-analysis)\n",
    "    - [Setting up](#Setting-up)\n",
    "    - [Data cleaning](#Data-cleaning)\n",
    "    - [Accuracy](#Accuracy)\n",
    "    - [Reaction Time](#Reaction-time)\n",
    "        - [Raw RT](#Raw-RT)\n",
    "        - [Normalized RT](#Normalized-RT)\n",
    "- [Conclusions & next steps](#Conclusions-&-next-steps)\n",
    "- [Important files](#Important-files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "0010-SRT-pilot was the first experiment Lissa and I ever conducted at Georgetown.  We wanted to see if we could replicate the Howard's SRT task in our own lab.  We began with this simple regular serial reaction time task (SRT), where every transition in the pattern was 100% predictable (i.e. a series of 8 button presses repeated over and over again).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials and method\n",
    "\n",
    "### Subjects\n",
    "- 20 GU undergrads (2 excluded for not finishing)\n",
    "- Conducted in the Newport Lab\n",
    "- Compensated $10 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materials\n",
    "- Hardware: Mac-mini, keyboard\n",
    "- Software: PsychoPy, Python\n",
    "- 4 circles aligned horizontally across the screen each of which corresponds to 4 keyboard buttons [z,x,n,m]\n",
    "- Each participant assigned to a series of 8 button presses that would repeat over and over (SRT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure\n",
    "- Circles fill green according to the serial pattern the participant was assigned to\n",
    "- Participants must press the keyboard button that corresponds to the illuminated circle.\n",
    "- They are instructed to work as quickly and as accurately as possible.\n",
    "- Every 80 trials (1 block) subjects are given a 60 second break.  They complete 20 blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and analysis\n",
    "\n",
    "Data analysis was conducted with R.  In this (and all) SRT experiments, we perform three main analyses.\n",
    "- Accuracy: Does participant accuracy change as a function of time?\n",
    "- Raw RT: Does raw reaction time change as a function of time?\n",
    "- Normalized RT: We z-transform as a way of normalizing RT data and analyze zRT as a function of time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up\n",
    "We begin by loading the required packages and setting up some figure parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the R libraries we need\n",
    "library(ggplot2)\n",
    "library(doBy)\n",
    "\n",
    "# adjust figure output size\n",
    "options(repr.plot.width = 5)\n",
    "options(repr.plot.height = 3)\n",
    "\n",
    "# define paths\n",
    "FIGPATH = '/Users/kathrynschuler/Documents/research-gu/figures/0010-srt-pilot-figs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "Before we compute accuracy and reaction time, we need to clean our data a little bit.  With reaction time data, there isn't really a standard procedure that everybody uses for eliminating outliers.  Some people use 2SD away from the mean; some people make a cutoff threshold (e.g. <1000ms) and remove any trials beyond it; some people normalize and then remove; etc.  In addition, the Howards like to remove participants who have accuracy levels below 80%, which is typically an indicator that they did not understand the task (or were not \"doing their best\")\n",
    "\n",
    "We will adopt this strategy here:\n",
    "- inspect the data visually to determine a reasonable cutoff criteria (usually 1000ms)\n",
    "    - the goal here is remove extreme values.\n",
    "- remove participants below 80% (to be consistent with the Howards)\n",
    "\n",
    "#### Outlier elimination\n",
    "Visually inspect the data and cutoff values beyond 1000ms if reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "orig.data = read.csv(\"processed-data/0010-srt-pilot-processed-data.csv\")\n",
    "\n",
    "# convert seconds to ms\n",
    "orig.data$RT = orig.data$RT*1000\n",
    "\n",
    "# boxplot to inspect for outliers\n",
    "ggplot(orig.data, aes(x=sid, y=RT)) + geom_boxplot(outlier.size = 1.5) + theme_classic(base_size=10) + \n",
    "    xlab('subject') + ylab('RT (ms)') + ggtitle('Distribution of reaction times by subject') + \n",
    "    theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))\n",
    "\n",
    "# save figure locally\n",
    "ggsave(paste(FIGPATH, '0010-distribution-rt-by-subject.png'), height = 3, width = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by looking at the data that there aren't any extremely ridiculous outliers.  In my opion, it actually makes more sense to leave these outliers in, rather than selecting some arbitrary cutoff criteria.  Later we are going to take the median RT, anyway, which is pretty resiliant to outliers.  These few outling points are hardly going to make a difference.  However, just to be consistent across studies, we will remove trials with RTs exceeding 1000ms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove trials with RT greater than or equal to 1000ms\n",
    "data.rm.extrmRT = subset(orig.data, RT <= 1000)\n",
    "\n",
    "# figure out how much data was lost\n",
    "n.outliers = nrow(orig.data) - nrow(data.rm.extrmRT)\n",
    "data.loss = n.outliers/nrow(orig.data) * 100 \n",
    "cat(\"Number of outliers: \", n.outliers, \"  Data loss: \", data.loss, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Remove participants with accuracy below 80%\n",
    "Next we will remove participants with overall accuracy below 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make function to compute percent correct (pcntc)\n",
    "pcnt.correct = function (x) return (sum(x)/length(x)*100)\n",
    "\n",
    "# get the overall accuracy for each subject and inspect\n",
    "overall.acc = summaryBy(isCorrect ~ sid, data = data.rm.extrmRT, FUN = pcnt.correct)\n",
    "overall.acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only one subject (S17) has overall accuracy below 80%.  Let's remove that participant to complete our data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove subjects below 80%\n",
    "data.rm.lowAcc = subset(data.rm.extrmRT, sid != \"S17\")\n",
    "\n",
    "# Check how much data was lost\n",
    "n.outliers = nrow(data.rm.extrmRT) - nrow(data.rm.lowAcc)\n",
    "data.loss = n.outliers/nrow(data.rm.extrmRT) * 100 \n",
    "cat(\"Number of outliers: \", n.outliers, \"  Data loss: \", data.loss, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this results in a bit more data loss (quite a bit in my opinion).  It is still a reasonable number, but I don't love it.  We could probably have left this person in without effecting our overall results at all.  Going forward, a better way of handling this is probably adding accuracy as predictor in our model (instead of excluding people based on accuracy criteria).  \n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Now we can compute mean accuracy by block for the remaining 19 subjects.  Note that the graph below is presenting values between 88 and 98%.  This is to help demonstrate some nuanced differences in accuracy across blocks (especially between blocks 1 and 2).  It is important to keep in mind that, in general, participants maintain extremely high accuracy across all blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute each subject's % correct by block\n",
    "acc.sid.block = summaryBy(isCorrect ~ sid*block, data = data.rm.lowAcc, FUN = pcnt.correct )\n",
    "\n",
    "# plot mean % correct by block\n",
    "ggplot(acc.sid.block, aes(factor(block), isCorrect.pcnt.correct)) +\n",
    "    stat_summary(fun.data = mean_se, size = 0.5, geom=\"pointrange\", position = \"dodge\") +\n",
    "    theme_classic(10) + theme(legend.position = \"bottom\") +\n",
    "    xlab(\"block\") + ylab(\"% correct\") + ggtitle(\"Percentage of correct trials by block\")\n",
    "# save figure locally\n",
    "ggsave(paste(FIGPATH, '0010-accuracy-by-block.png'), height = 3, width = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reaction time \n",
    "\n",
    "\n",
    "Next we will analyze the reaction time data for our remaining 19 subjects.  To do so, we need to first do a bit more data cleaning (Howard & Howard, 1997)\n",
    "\n",
    "#### Remove incorrect trials\n",
    "We need to remove any trials that were incorrect.  Although these might be informative in some way, they are impossible to interpret. (Did the subject make a mistake completely irrelevant to our manipulation? or did the subject make a mistake that tells us what he/she was predicting to happen, even though it wasn't exactly right?)  For now, we will remove them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# used the previously cleaned data set and additionally remove incorrect trials\n",
    "data.rm.incorrect = subset(data.rm.lowAcc, isCorrect == 1)\n",
    "\n",
    "# Check how much data was lost \n",
    "n.outliers = nrow(data.rm.lowAcc) - nrow(data.rm.incorrect)\n",
    "data.loss = n.outliers/nrow(data.rm.lowAcc) * 100 \n",
    "cat(\"Number of outliers: \", n.outliers, \"  Data loss: \", data.loss, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw RT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cleaned data set, we calcuate median RT for each participant for each block and then take the mean of those median RTs (Howard & Howard, 1997) .\n",
    "\n",
    "##### Median RT by subject by block\n",
    "We can plot the median RT for each participant for each block, which gives us each participants learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# individual subject data for mean of median RT\n",
    "medianRT.block = summaryBy(RT ~ sid*block, data = data.rm.incorrect, FUN = median, var.name = \"block\")\n",
    "\n",
    "# plot median RT by subject\n",
    "ggplot(medianRT.block, aes(block, block.median, ymax = max(block.median))) + \n",
    "       geom_point(stat=\"identity\", position = \"dodge\", size = 1) +\n",
    "       facet_wrap(~sid) + theme_classic(base_size=8) + ylab(\"median RT (ms)\") +\n",
    "       ggtitle(\"Median reaction time by block for individual subjects\")\n",
    "\n",
    "ggsave(paste(FIGPATH, '0010-median-RT-by-subject-raw.png'), height = 3, width = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great - we can see that each participant has a different learning curve, but the overall pattern is the similar (they get faster over time).  However, it also demonstrates the utility of normalizing the RTs. The Howard & Howard analysis method takes the mean of these median reaction time to give the final data used for analysis.  Normalizing the RTs might help this situation.  (However, going forward we might want to use a mixed-effect model)\n",
    "\n",
    "##### Mean of median RT\n",
    "\n",
    "Now we can compute the mean of median reaction times by block to show the group learning effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the mean of median RT\n",
    "ggplot(medianRT.block, aes(factor(block), block.median)) + \n",
    "    stat_summary(fun.data=mean_se, size = 0.5, geom= \"pointrange\", position = \"dodge\") +\n",
    "    theme_classic(base_size=10) + ylab(\"median RT (ms)\") + xlab(\"block\") +\n",
    "    ggtitle(\"Mean of median reaction time by block\")\n",
    "\n",
    "ggsave(paste(FIGPATH, '0010-mean-of-median-RT-raw.png'), height = 3, width = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Normalized RT\n",
    "\n",
    "The most popular options for normalizing RT data in the literature are proportional transformations and z-score transformations.  Z-score transforms are preferred (by us and other labs), because proportional transforms are not as good at controlling for differences in processing speed (age-related or otherwise).  This is because proportional transformations assume that the function relating processing speed between two groups is linear, with an intercept of zero, which is not always valid (e.g. Chris, et al, 2001; Faust, Balota, Speiler, & Ferrar, 1999).\n",
    "\n",
    "##### Compute median modified z-score\n",
    "To compute z-score, we follow what has been suggested previously (e.g. Praat, Abrams, & Chasteen, 1997), with modifications for computing z-scores based on median and median absolute deviations (rather than mean and standard deviation):\n",
    "\n",
    " - For each participant, we computed an overall median RT and median absoulte deviation (MAD)\n",
    " - Then, we subtracted this overall median RT from the participant’s median RT for each block.\n",
    " - We multiplied this result by the constant 0.6745, as recommended by Iglewicz & Hoaglin (1993) for z-scored medians.\n",
    " - Then we divided this value by the participant’s overall median absolute deviation (MAD) (again, as Iglewicz & Hoaglin suggest).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute overall median RT and MAD from correct trials\n",
    "medianRT.overall = summaryBy(RT ~ sid, data = data.rm.incorrect, FUN = list(median, mad), var.name = \"overall\")\n",
    "\n",
    "# subtract overall median RT from block median RT by participant\n",
    "z.data = merge(medianRT.block, medianRT.overall, by = 'sid')\n",
    "\n",
    "# compute median modified z-score\n",
    "z.data$z.RT = (0.6745*(z.data$block.median - z.data$overall.median))/z.data$overall.mad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Median RT by subject by block\n",
    "Now we can once again plot by block for each individual, this time with the normalized RT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot median RT by subject\n",
    "ggplot(z.data, aes(block, z.RT, ymax = max(z.RT))) + \n",
    "       geom_point(stat=\"identity\", position = \"dodge\", size = 1) +\n",
    "       facet_wrap(~sid) + theme_classic(base_size=8) + ylab(\"median RT (ms)\") +\n",
    "       ggtitle(\"Median z-score RT by block for individual subjects\")\n",
    "\n",
    "ggsave(paste(FIGPATH, '0010-median-RT-by-subject-zscore.png'), height = 3, width = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Mean of median RT\n",
    "Now we can compute the mean of these z transformed median reaction times by block to show the group learning effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the mean of z RT\n",
    "ggplot(z.data, aes(factor(block), z.RT)) + \n",
    "    stat_summary(fun.data=mean_se, size = 0.5, geom= \"pointrange\", position = \"dodge\") +\n",
    "    theme_classic(base_size=10) + ylab(\"median RT (ms)\") + xlab(\"block\")+\n",
    "    ggtitle(\"Mean of median z-score RT by block\")\n",
    "\n",
    "ggsave(paste(FIGPATH, '0010-mean-of-median-RT-zscore.png'), height = 3, width = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions & next steps\n",
    "\n",
    "### People can learn this (at different rates)\n",
    "Now let's take a look at the overal median and mad for each partipant, just to inspect things a little bit.  We can see in the table below that the overal medians are widely variable for each participant. Some of the overall median values are in fact extremely fast. S06 has an overall median of 65.51 ms, for example, well below the phisiologically possible RT to a visual stimulus (typcially 100ms in humans).  What does this mean?  First, RTs below 100ms typically indiate **anticipatory** reaction times.  The fact that S06 (and others) had anticipatory reaction times as their overall reaction time tells us that they learned extremely fast.  This makes a lot of sense when you think about how simple the actual task was (the same exact 4 button sequence repeated over and over for 1600 trials).    Participants who have much longer overall reaction times also tend to have much larger median absoulte deviations, meaning that their reaction times are significantly changing over blocks - suggesting they take longer to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "medianRT.overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "We acheived our original goal of replicating an SRT paradigm.  We know that Katie's paradigm is working, and we can move on to replicating the alternating serial reaction time task. (aSRT task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important files\n",
    "Files can be found on Katie's local computer at the path described, or you can access it on dropbox by clicking the link. All local paths are relative to `/Users/kathrynschuler/Documents/research-gu/`\n",
    "\n",
    "- [Summary](): `summaries/0010-srt-pilot.pdf`\n",
    "- [Stimuli](https://www.dropbox.com/sh/zfs3kwopxgktvff/AAA665qHEAr6Rp4DEB4nMH9ga?dl=0): `stimuli/0010-str-pilot-stims/`\n",
    "- [Experiment](https://www.dropbox.com/sh/3zrqr371ldw5gu0/AAAjxrHccjPTZ-Re_0dZD-SGa?dl=0): `experiment-code/0010-srt-pilot-exp/`\n",
    "- [Subject tracking](https://www.dropbox.com/s/lak50vpz35yx5he/0010-SRT-pilot-track.csv?dl=0): `subject-tracking/0010-srt-pilot-track.csv`\n",
    "- [Raw data](https://www.dropbox.com/sh/m1ixi64n28j4k8t/AADeh5hFw25EvvfVMtOxZhwRa?dl=0): `raw-data/0010-srt-pilot-data`\n",
    "- [Analysis](https://www.dropbox.com/sh/8m5td0y7w39ox6j/AACp3fAxtvg9v1_WyPpNe7MCa?dl=0): `analyses/0010-srt-pilot-analysis/`\n",
    "- [Figures](https://www.dropbox.com/sh/ex3ma6g4u124bi7/AACZct7dUlb4hfPqruhaoa1oa?dl=0): `figures/0010-srt-pilot-figs/`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
